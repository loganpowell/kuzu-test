#!/usr/bin/env python3
"""
Generate cross-platform comparison report for KuzuDB (Python vs Node.js vs WASM) (DEPRECATED)

NOTE: Use generate_comprehensive_report.py instead, which includes cross-platform
comparison in a single comprehensive report with table of contents.
"""

import json
from pathlib import Path


def load_results():
    """Load all benchmark results."""
    results_dir = Path(__file__).parent.parent / "results"

    results = {}

    # Load Python results
    python_loading = results_dir / "loading_benchmark_results.json"
    if python_loading.exists():
        with open(python_loading) as f:
            results["python_loading"] = json.load(f)

    python_queries = results_dir / "query_benchmark_results.json"
    if python_queries.exists():
        with open(python_queries) as f:
            results["python_queries"] = json.load(f)

    # Load Node.js results
    nodejs_loading = results_dir / "nodejs_loading_benchmark_results.json"
    if nodejs_loading.exists():
        with open(nodejs_loading) as f:
            results["nodejs_loading"] = json.load(f)

    # Load library sizes
    lib_sizes = results_dir / "library_sizes.json"
    if lib_sizes.exists():
        with open(lib_sizes) as f:
            results["library_sizes"] = json.load(f)

    # Load WASM results (find most recent)
    wasm_files = list(results_dir.glob("kuzu-wasm-benchmark-*.json"))
    if wasm_files:
        latest_wasm = max(wasm_files, key=lambda p: p.stat().st_mtime)
        with open(latest_wasm) as f:
            results["wasm"] = json.load(f)

    return results


def generate_comparison_report(results):
    """Generate cross-platform comparison report."""

    report = []
    report.append("=" * 80)
    report.append("KuzuDB Cross-Platform Comparison Report")
    report.append("=" * 80)
    report.append("")

    # Platform Overview
    report.append("## Platform Overview")
    report.append("")
    report.append("| Platform | Status | Best For |")
    report.append("|----------|--------|----------|")
    report.append("| Python | ‚úÖ Production Ready | Server-side auth, stable |")
    report.append("| Node.js | ‚ö†Ô∏è Stability Issues | Fast loading, needs fixes |")
    report.append("| WASM | ‚úÖ Working | Browser-based auth, offline |")
    report.append("")

    # Bundle/Library Size Comparison
    report.append("## Bundle/Library Size Comparison")
    report.append("")
    report.append("| Platform | Total Size | Notes |")
    report.append("|----------|-----------|-------|")

    if "library_sizes" in results and results["library_sizes"].get("python"):
        py_size = results["library_sizes"]["python"]["total_formatted"]
        report.append(f"| Python | {py_size} | Smallest, single platform |")

    if "library_sizes" in results and results["library_sizes"].get("nodejs"):
        node_total = results["library_sizes"]["nodejs"]["total_formatted"]
        node_single = results["library_sizes"]["nodejs"].get("native_formatted", "N/A")
        report.append(
            f"| Node.js | {node_total} | All platforms: {node_total}, Single: ~17MB |"
        )

    if "wasm" in results and "bundleSize" in results["wasm"]:
        wasm_size = results["wasm"]["bundleSize"]["totalFormatted"]
        report.append(f"| WASM | {wasm_size} | Browser bundle (approximate) |")

    report.append("")
    report.append("**Winner**: Python (smallest footprint)")
    report.append("")

    # Loading Performance Comparison
    report.append("## Loading Performance Comparison")
    report.append("")
    report.append("**Note**: Different dataset sizes across platforms")
    report.append("")
    report.append("| Platform | Dataset | Load Time | Throughput | Memory |")
    report.append("|----------|---------|-----------|------------|--------|")

    # Python (CSV)
    if "python_loading" in results:
        for result in results["python_loading"]:
            if result["format"] == "csv":
                report.append(
                    f"| Python | 34,567 records | {result['total_load_time_sec']:.3f}s | "
                    f"{result['records_per_second']:,.0f}/s | {result['memory_used_mb']:.1f}MB |"
                )
                break

    # Node.js (CSV)
    if "nodejs_loading" in results:
        for result in results["nodejs_loading"]:
            if result["format"] == "csv":
                report.append(
                    f"| Node.js | 34,567 records | {result['total_load_time_sec']:.3f}s | "
                    f"N/A | {result.get('memory_used_mb', 'N/A')} |"
                )
                break

    # WASM
    if "wasm" in results and "loading" in results["wasm"]:
        wasm_load = results["wasm"]["loading"]
        report.append(
            f"| WASM | {wasm_load['totalRecords']:,} records | "
            f"{wasm_load['loadTimeMs'] / 1000:.3f}s | "
            f"{wasm_load['recordsPerSecond']}/s | "
            f"{results['wasm']['memory']['usedFormatted']} |"
        )

    report.append("")
    report.append("**Analysis**:")
    report.append("- Node.js: Fastest for large datasets")
    report.append("- WASM: Slower due to row-by-row inserts (no bulk COPY FROM)")
    report.append("- Python: Good balance of speed and stability")
    report.append("")

    # Query Performance Comparison
    report.append("## Query Performance Comparison")
    report.append("")

    # Get comparable queries
    query_comparison = {}

    if "python_queries" in results:
        for q in results["python_queries"]:
            name = q["query_name"]
            if "Direct Permission Check" in name:
                query_comparison.setdefault("Direct Permission", {})["Python"] = q[
                    "avg_time_ms"
                ]
            elif "Group-Based Permission" in name:
                query_comparison.setdefault("Group Permission", {})["Python"] = q[
                    "avg_time_ms"
                ]
            elif "User's Groups" in name:
                query_comparison.setdefault("User Groups", {})["Python"] = q[
                    "avg_time_ms"
                ]

    if "wasm" in results and "queries" in results["wasm"]:
        for q in results["wasm"]["queries"]:
            name = q["name"]
            if "Direct Permission" in name:
                query_comparison.setdefault("Direct Permission", {})["WASM"] = q[
                    "avgMs"
                ]
            elif "Group-Based Permission" in name:
                query_comparison.setdefault("Group Permission", {})["WASM"] = q["avgMs"]
            elif "User Groups" in name:
                query_comparison.setdefault("User Groups", {})["WASM"] = q["avgMs"]

    report.append("| Query Pattern | Python | WASM | Winner |")
    report.append("|---------------|--------|------|--------|")

    for query_name, platforms in query_comparison.items():
        py_time = platforms.get("Python", "N/A")
        wasm_time = platforms.get("WASM", "N/A")

        if isinstance(py_time, (int, float)) and isinstance(wasm_time, (int, float)):
            winner = "Python" if py_time < wasm_time else "WASM"
            report.append(
                f"| {query_name} | {py_time:.2f}ms | {wasm_time:.2f}ms | {winner} |"
            )
        else:
            report.append(f"| {query_name} | {py_time} | {wasm_time} | - |")

    report.append("")
    report.append("**Winner**: Python (consistently faster queries)")
    report.append("")

    # Memory Usage
    report.append("## Memory Usage Comparison")
    report.append("")
    report.append("| Platform | Memory Usage | Notes |")
    report.append("|----------|--------------|-------|")

    if "python_loading" in results:
        for result in results["python_loading"]:
            if result["format"] == "csv":
                report.append(
                    f"| Python | {result['memory_used_mb']:.1f}MB | During CSV load |"
                )
                break

    if "wasm" in results and "memory" in results["wasm"]:
        wasm_mem = results["wasm"]["memory"]
        report.append(f"| WASM | {wasm_mem['usedFormatted']} | JS Heap (browser) |")

    report.append("")

    # Use Case Recommendations
    report.append("## Platform Recommendations by Use Case")
    report.append("")
    report.append("### ‚úÖ Use Python When:")
    report.append("- Building server-side authorization systems")
    report.append("- Need production stability and maturity")
    report.append("- Want smallest library footprint (11.84 MB)")
    report.append("- Need best query performance (<2ms average)")
    report.append("- Using Parquet for optimal loading")
    report.append("")
    report.append("### ‚úÖ Use WASM When:")
    report.append("- Building browser-based applications")
    report.append("- Need offline/client-side authorization")
    report.append("- Want to avoid server round-trips")
    report.append("- Working with smaller datasets (<10K records)")
    report.append("- Building privacy-focused applications")
    report.append("")
    report.append("### ‚ö†Ô∏è Use Node.js When:")
    report.append("- Need fastest loading speed (44% faster than Python)")
    report.append("- Stability issues are acceptable")
    report.append("- Can wait for more mature bindings")
    report.append("")

    # Overall Verdict
    report.append("## Overall Platform Verdict")
    report.append("")
    report.append("| Criterion | Winner | Reason |")
    report.append("|-----------|--------|--------|")
    report.append("| **Query Speed** | ü•á Python | 1.7ms avg, most consistent |")
    report.append("| **Loading Speed** | ü•á Node.js | 0.286s (44% faster) |")
    report.append("| **Bundle Size** | ü•á Python | 11.84 MB (smallest) |")
    report.append("| **Stability** | ü•á Python | Production ready |")
    report.append("| **Browser Support** | ü•á WASM | Only option for browsers |")
    report.append("| **Offline Use** | ü•á WASM | Works without server |")
    report.append("")
    report.append("### üèÜ Overall Winner: **Python**")
    report.append("")
    report.append("**Rationale**: Python offers the best combination of:")
    report.append("- Excellent query performance (1.7ms avg)")
    report.append("- Small footprint (11.84 MB)")
    report.append("- Production-ready stability")
    report.append("- Best for server-side authorization")
    report.append("")
    report.append("**Second Place**: WASM for browser/offline use cases")
    report.append("**Third Place**: Node.js (needs stability improvements)")
    report.append("")

    # WASM Specific Insights
    if "wasm" in results:
        report.append("## WASM Performance Insights")
        report.append("")
        report.append("### Loading Performance")
        wasm_load = results["wasm"]["loading"]
        report.append(f"- Dataset: {wasm_load['totalRecords']:,} records")
        report.append(f"- Load Time: {wasm_load['loadTimeMs'] / 1000:.2f}s")
        report.append(f"- Throughput: {wasm_load['recordsPerSecond']} records/sec")
        report.append("")
        report.append("**Note**: WASM is slower because it inserts row-by-row")
        report.append("(no bulk COPY FROM in browser context)")
        report.append("")

        report.append("### Query Performance")
        for q in results["wasm"]["queries"]:
            report.append(f"- {q['name']}: {q['avgMs']:.2f}ms avg")
        report.append("")
        report.append(
            "**Analysis**: WASM queries are ~2-3x slower than Python but still"
        )
        report.append("fast enough for real-time authorization (<5ms for most queries)")
        report.append("")

        report.append("### Memory Footprint")
        mem = results["wasm"]["memory"]
        report.append(f"- Used Heap: {mem['usedFormatted']}")
        report.append(f"- Total Heap: {mem['totalFormatted']}")
        report.append(f"- Heap Limit: {mem['limitFormatted']}")
        report.append("")
        report.append("**Analysis**: Reasonable memory usage for browser context")
        report.append("")

    report.append("=" * 80)
    report.append("End of Cross-Platform Comparison Report")
    report.append("=" * 80)

    return "\n".join(report)


def main():
    """Generate and save cross-platform comparison report."""
    print("Generating cross-platform comparison report...")

    results = load_results()
    report = generate_comparison_report(results)

    # Save report
    output_dir = Path(__file__).parent.parent / "results"
    output_file = output_dir / "CROSS_PLATFORM_COMPARISON.md"

    with open(output_file, "w") as f:
        f.write(report)

    print(f"\n‚úÖ Report generated: {output_file}")
    print("\nReport preview:")
    print("-" * 80)
    print(report)


if __name__ == "__main__":
    main()
